# Parcel Crawl Service — How to Interface

Purpose: submit an address/design (DXF + optional captured footprint) and run parcel_crawl_demo_v4 across parcels. This service wraps the crawler with upload, shrink-wrap, job execution, and artifact download APIs.

Base URLs
- Production: https://landlens.up.railway.app
- Local dev: http://localhost:8000
- OpenAPI docs: /docs
- Schema: /openapi.json

Typical Workflow
1) Upload DXF: POST /files (multipart `file=@...`). Response includes `file_url` (use as `dxf_url` later) and public `download_url`.
2) (Optional) Prepare footprint/frontage locally: POST /files/{name}/shrinkwrap with rectangle + frontage points, or GET /files/{name}/preview to auto-detect. This yields `footprint_points` and `front_direction` you can reuse in jobs.
3) Submit job: POST /jobs with address, `dxf_url` (from step 1), and optional `config` including `footprint_points`/`front_direction` from step 2. Service queues the job and runs parcel_crawl_demo_v4.
4) Monitor: GET /jobs/{job_id} for status/result; GET /jobs/{job_id}/logs for crawl.log tail; GET /jobs/{job_id}/artifacts for manifest + download URLs; GET /jobs/{job_id}/geo for GeoJSON footprints + progress.
5) Download outputs: GET /jobs/{job_id}/outputs/{path} as returned in artifacts, or use the URLs injected in the artifact manifest.

Endpoint Details
- GET /health — service status and upload target info.

Jobs
- POST /jobs — submit a crawl job.
  Body fields:
    address (string) — human-readable address for context/logging.
    dxf_url (string) — http(s) or file:// URL to DXF. Use the `file_url` returned by /files.
    config (object, optional) — passed through to parcel_crawl_demo_v4, e.g. cycles, score_workers, rotation_step, auto_front (bool), auto_offset (bool), render_cycle/render_best/render_composite (bool), full_rotation, skip_roads, front_angle, buffer, setback, etc.
    footprint_points (array, optional) — list of [x, y] points defining the footprint polygon (from shrinkwrap/preview).
    front_direction (array, optional) — [x, y] vector for frontage heading.
  Response: { id, status }
- GET /jobs — list in-memory jobs.
- GET /jobs/{job_id} — job record with status, any error/result_url, and manifests stored when complete.
- GET /jobs/{job_id}/logs — tail of crawl.log (default 200 lines; `?lines=` up to 2000).
- GET /jobs/{job_id}/artifacts — artifact manifest with download URLs injected for files under job outputs or uploads.
- GET /jobs/{job_id}/geo — GeoJSON FeatureCollection of best footprints per parcel plus progress counts.
- POST /jobs/{job_id}/cancel — cancel a queued/running job. Body (optional): `{ "reason": "user cancelled from dashboard" }`. Response reports whether the worker was already running; if so, status becomes `cancelling` until the worker observes the cancellation and exits.

Job outputs (from parcel_crawl_demo_v4)
- Stored under storage/jobs/{job_id}/outputs.
- Artifacts include: best_parcels.json; cycles/cycle_*.json and cycle_*.png; parcels/<parcel_id>/{best.png, composite.png, placements.json}.
- Manifest URLs let you fetch PNG/JSON directly via /jobs/{job_id}/outputs/... (proxy) or via the injected URLs.

File Uploads & Prep
- POST /files — upload DXF. Multipart field `file`; optional `filename` query. Returns stored path, file:// URL, download URL, and any extracted files if a ZIP.
- GET /files — list uploaded files with file:// and download URLs.
- GET /files/{filename} — download raw file.
- DELETE /files/{filename} — delete uploaded file.
- GET /files/{filename}/geometry — raw DXF polylines/paths (for custom previews).
- GET /files/{filename}/preview — auto-detect footprint + front_direction from DXF.
- POST /files/{filename}/shrinkwrap — run shrink-wrap from user-picked rectangle/front points. Body: { "rectangle_points": [[x,y] x3], "front_points": [[x,y],[x,y]] }. Returns footprint_points, front_direction, front_origin, area. Use these in /jobs config.

Designs
- GET /designs — list saved design JSON files from storage/designs.
- POST /designs — save a design payload: requires name, dxf_url, footprint_points, front_direction.
- GET /designs/{slug} — fetch a saved design.

Geocode
- GET /geocode/reverse?lat=&lon= — reverse geocode via OSM Nominatim.

Debug
- GET /debug/env — selected environment details (for troubleshooting).

Notes
- Authentication: none enabled by default; restrict at ingress if needed.
- Storage: job workspaces under storage/jobs; uploads under /data (override with DXF_UPLOAD_ROOT).
- Worker uses `parcel_crawl_demo_v4.py` with flags built from `config` and the provided footprint/frontage when supplied.
- Monitoring tips:
  - `GET /jobs/{id}` includes `log_tail` so you can see the last lines from `crawl.log`. Use this to ensure the worker started and to check progress without separate log calls.
  - `GET /jobs/{id}/geo` only returns features once the crawler writes outputs. If it’s empty and `log_tail` shows no activity, the job may still be queued or waiting for the worker. If `log_tail` shows cycles progressing but no GeoJSON, wait for the first cycle to finish.
  - Cancel stuck jobs via `POST /jobs/{id}/cancel`; check `log_tail` first to avoid cancelling an active run mid-progress.
- Concurrency: the API runs up to `API_JOB_WORKERS` crawl jobs in parallel (default 2). Additional submissions queue until a worker thread is free.
